{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Parallel Computing \n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture, we'll discuss parallel computing. We'll first lok at the reasons for parallel computing and then looks at some data-science specific parallel applications such as MapReduce and Spark. \n",
    "\n",
    "**Further reading:**\n",
    "\n",
    "[J. Lin and C. Dyer, Data-Intensive Text Processing with MapReduce (2010)](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is parallelism?\n",
    "\n",
    "Traditional algorithms use serial computation, which means that instructions are processed one after the others. These algorithms are easy to implement.\n",
    "\n",
    "**Parallelism** is the approach of executing multiple threads simultaneously. This can be applied to anything. For example, it is common that GUI applications have a thread for the user interface and a separate thread for any back-end processing (saving a file, running a computation), which ensures that the user interface stays responsive while the back-end processing is active. \n",
    "\n",
    "**Parallel computing** is when a computational task is broken into smaller subtasks, which are processed simultaneously and indpendently. \n",
    "\n",
    "So why do we need parallelism? Speed! The benefits of parallel computing are that either larger tasks can be completed that couldn't be completed in reasonable time on a single CPU, or that tasks can be completed faster. \n",
    "\n",
    "## Group Exercise\n",
    "\n",
    "```\n",
    "R1: 19 18 7 13 17\n",
    "R2: 19 18 18 11 7 \n",
    "R3: 9 1 5 10 13\n",
    "R4: 20 7 17 17 6 \n",
    "R5: 14 18 5 13 14\n",
    "R6: 19 2 5 11 1  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rise of Parallelism\n",
    "\n",
    "Parellism has always been important, but it is becoming increasingly relevant. Two decades ago, we could rely on increasing CPU speeds to make computation faster and faster. [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law) states that the number of transistors in a chip doubles approximately every two years. And more transistors (naively) equals more computing power.\n",
    "\n",
    "![Moore's Law](moores_law.png)\n",
    "\n",
    "Up to about 2004 this also correlated with an increase in speed for a single CPU core by increasing it's clock speed (number of instructions computed per second). However, since then it hasn't been possible to further increase clock speed for various technical reasons. \n",
    "\n",
    "![Clock Speed](clock_speed.png)\n",
    "\n",
    "Now, instead of making individual CPUs faster, we are adding more CPUs on a single Chip. However, having multiple processors doesn't make a single thread faster. Hence, we increasingly need parallelism to leverage the speed advantages. \n",
    "\n",
    "On a larger scale, we can also distribute workload amonst multiple computers, e.g., in a computer clusters. Also, GPUs (Graphics Processing Units) are now used for a wide variety of tasks, because of their ability to run many processes in parallel. \n",
    "\n",
    "**A Programmers Perspective.** Question: How do you make your program run faster? \n",
    "\n",
    "Answer before 2004: Wait 6 months and buy a new computer. \n",
    "\n",
    "Answer after 2004: You need to write parallel software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:** To speed up the addition of an array of numbers, we can leverage parallelism:\n",
    "\n",
    "![GPGPU Example](gpgpu.png)\n",
    "*Source: Kohei KaiGai*\n",
    "\n",
    "\n",
    "**Example 2:** To use cross validation to search for optimal parameters, you could have put each parameter tested on a different computer. See the `n_jobs` parameter in the scikit-learn command [*GridSearchCV*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "### Parallel Thinking\n",
    "\n",
    " * Decompose work into pieces that can safely be performed in parallel\n",
    " * Assign work to processors\n",
    " * Managing communication/syncrhonization between the processors so that it does not limit speedup. \n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In a group, develop conceptual strategies to parallelize these operations for the following tasks. Think about how to split the input (you want to split evenly with respect to workload) and how to merge the data. \n",
    "\n",
    " * Multiply all elements of a large array by 4.\n",
    " * Calcualte the mean of a very large array.\n",
    " * Check all numbers from one to a hundred thousand for primality.\n",
    "\n",
    "Which of these operations is better to parralelize? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurdles\n",
    "\n",
    "On the other hand, parallel computing is not easy. There are a large number of low-level programming aspects that must be handled. For example, one must consider  \n",
    "- Partitioning input data\n",
    "- Shared memory (Open Multiprocessing (OpenMP)) or distributed memory (Message Passing Interface (MPI)) architectures\n",
    "- Scheduling execution\n",
    "- Handling failures\n",
    "- Interprocessor communication\n",
    "\n",
    "There are a lot of difficult Computer Science questions here! For example, [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) gives the theoretical speedup of a process depending on how much of the task can be parallelized. \n",
    "\n",
    "![Amdahl's Law](amdahls_law.png)\n",
    "\n",
    "*Source: Daniels220, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=6678551*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parallelism in Python\n",
    "\n",
    "Python has support for low-level parallelism; see the \n",
    "[python documentation](https://docs.python.org/3.6/library/concurrency.html), \n",
    "[multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html),\n",
    "[ipyparallel](https://ipyparallel.readthedocs.io/en/latest/index.html), \n",
    "[joblib](https://pypi.python.org/pypi/joblib), \n",
    "*etc*..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll use the multiproceesing library for an example. It uses lower level process spawning API and provides true parellelism by spawning sub processes. It uses the multiple processor cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    prime = True\n",
    "    for i in arange(2, n):\n",
    "        if n/i % 1 == 0:\n",
    "            prime = False\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will call the function isprime for integers 1 to max_number. This will be carried out serially one number at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prime_serial_test(max_number):\n",
    "    [isprime(i) for i in range(max_number)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a pool of processes and will call isprime on integers from 1 to max_number. Multiple numbers will be processed at the same time, depending on the number of processes and the number of CPUs in your computer. \n",
    "\n",
    "Tthe number of processes spawned in the Pool must be less than or equal to number of cores in CPU for this to make sense. Using more processes than CPUs reduces the performance rather than improving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prime_parallel_test(max_number):\n",
    "    # create multiple sub-processes\n",
    "    pool = multiprocessing.Pool(processes = 4)\n",
    "    # pass a chunk of numbers into each process as it becomes available\n",
    "    # the chunksize defines how many are put in in batch\n",
    "    pool.map(isprime, range(max_number), chunksize=100)\n",
    "    # pool.close() will terminate all the subprocesses once their allocated work is done.\n",
    "    pool.close()\n",
    "    # pool.join() makes the main processes wait for subprocesses to complete\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.67 s, sys: 60.2 ms, total: 4.73 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%time prime_serial_test(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 ms, sys: 20.1 ms, total: 33.9 ms\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%time prime_parallel_test(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results on a machine with 4 physical cores (8 - logical cores due to hyperthreading).\n",
    "\n",
    "| Condition                         | Time     |\n",
    "| :---------------------------------|  --------|\n",
    "| Serial                            | 4.23 sec |\n",
    "| Parallel  (with 2 processes)      | 2.59 sec |\n",
    "| Parallel  (with 3 processes)      | 2.18 sec | \n",
    "| Parallel  (with 4 processes)      | 1.88 sec |\n",
    "| Parallel  (with 8 processes)      | 1.91 sec |\n",
    "| Parallel  (with 16 processes)     | 2.06 sec |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the sum of all pairwise products: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.49 s, sys: 5.54 ms, total: 1.5 s\n",
      "Wall time: 1.5 s\n",
      "6191253.2464611\n",
      "CPU times: user 11.6 ms, sys: 5.18 ms, total: 16.8 ms\n",
      "Wall time: 840 ms\n",
      "6191253.2464611\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "big_array = [random.random() for _ in range(5000)]\n",
    "big_array[:10]\n",
    "# This function multiplies each element of vector X with element y\n",
    "# and returns sum of the products.\n",
    "def ssum(X, y):\n",
    "    return sum(x*y for x in X)\n",
    "\n",
    "# This function calls ssum for each element of vector X with X itself.\n",
    "def pw_sum(X):\n",
    "    return sum(ssum(X,y) for y in X)\n",
    "    \n",
    "# This function makes it parallelizes the pw_sum function.\n",
    "# pool.map can only take a function and one argument for the function.\n",
    "# To pass two arguments we turn them into a single tuple.\n",
    "# We pass it to ssum using *args which is variable arguments in python, this automatically unfolds the tuple into arguments of ssum.\n",
    "def parallel_pw_sum(args):\n",
    "    return ssum(*args)\n",
    "\n",
    "# Serial call for pairwise sum.\n",
    "%time results = pw_sum(big_array)\n",
    "print(results)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "# Notice how we pass the list to parallel_pw_sum with each element of list being a tuple as discussed in above comment.\n",
    "%time results = pool.map(parallel_pw_sum, ((big_array,y) for y in big_array))\n",
    "print(sum(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Object Oriented Programming\n",
    "\n",
    "Object-oriented programming is an important paradigm that we've largely ignored so far. See also [the documentation](https://docs.python.org/3/tutorial/classes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The class definition\n",
    "class Box():\n",
    "    # initialization\n",
    "    def __init__(self):\n",
    "        # this is how you initialize class members\n",
    "        self.nr_objects = 0\n",
    "    # a method\n",
    "    def isEmpty(self):\n",
    "        if self.nr_objects:\n",
    "            return False;\n",
    "        return True;\n",
    "    \n",
    "    def putObjectsIn(self, nr_objects):\n",
    "        self.nr_objects += nr_objects;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_box = Box()\n",
    "my_box.isEmpty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_box.putObjectsIn(3)\n",
    "my_box.isEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheritance\n",
    "\n",
    "We can use inheritance to make a more specialized version of a class. Here, we make a smarter Box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The parent class is passed in parantheses of the class definition\n",
    "class SmartBox(Box):\n",
    "    # we add a new method that is not in the original Box class\n",
    "    def howManyObjects(self):\n",
    "        return self.nr_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_box = SmartBox()\n",
    "# the methods of the Box base class are available in a SmartBox\n",
    "smart_box.putObjectsIn(5)\n",
    "smart_box.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the new method is available as well\n",
    "smart_box.howManyObjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding a function\n",
    "\n",
    "Sometimes it makes sense to replace the functionality of a base class with a specific implementation for the new class. We call this overriding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrintingBox(Box):\n",
    "    def isEmpty(self):\n",
    "        if self.nr_objects:\n",
    "            print(\"Your Box is not empty.\")\n",
    "            return False;\n",
    "        print(\"Your Box is empty.\")\n",
    "        return True;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same call – `isEmpty()` also prints something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Box is empty.\n",
      "Your Box is not empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printing_box = PrintingBox()\n",
    "printing_box.isEmpty()\n",
    "printing_box.putObjectsIn(9)\n",
    "printing_box.isEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Write a class `Dog` that has an array of tricks (strings). Add and add_trick function. Initialize a couple of dogs and add different tricks. Print which tricks each dog can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "1. Programming model for  distributed computations\n",
    "+ Addressing large data sets (think ~ 1 terabyte of data)\n",
    "+ Parallel and distributed algorithm\n",
    "+ Cluster framework\n",
    "+ Functional\n",
    "\n",
    "**History:**\n",
    "\n",
    "1. Developed by Google, but built on previously-developed ideas\n",
    "+  Apache Hadoop is an open source implemenation \n",
    "+ implemented in Java\n",
    "+ There are several Python interfaces to Hadoop, including MrJob, etc.... Unfortunately, these don't yet work nicely with Jupyter notebook. \n",
    "\n",
    "**Concept:**\n",
    "\n",
    "Data is typically stored in key-value pair. The key is a unique (or to some resolution) identifier of the object, and then the value can store anything.\n",
    "\n",
    "1. Mapping: \n",
    "Typically, the initial format of the key-value pair is very rough. The data is just stored, but\n",
    "not processed much yet, and in any particular order. It may contain many irrelevant parts of information (for\n",
    "the current task). So the mapping phase is in charge of getting it into the right format. Keep in mind, this\n",
    "data set is very large, so it is stored once, but may be used for many many different purposes.\n",
    "The mapping phase takes a file, and converts into another set of key-value pairs. The values generally\n",
    "contain the data of current interest, and the keys are used to obtain locality in the next part.\n",
    "\n",
    "2. Shuffling: \n",
    "Output of Map is typically (roughly) as large as the original data, so it also cannot all fit on\n",
    "one machine. The shuffle step puts all key-value pairs with the same key on one machine (if they can all\n",
    "fit). The data is redistributed and mixed up, kind of what it would look like in the mapping of card positions\n",
    "when shuffling a deck of cards...\n",
    "\n",
    "3. Reducing: \n",
    "The reduce step takes the data that has been aggregated by keys and does something useful\n",
    "(application specific). This data is now all in the same location – we have locality.\n",
    "\n",
    "![](map_reduce.png)\n",
    "\n",
    "\n",
    "-See Lecture 14 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k) and the accompanying video [here](http://cs109.github.io/2015/pages/videos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's looks at a practical example. Note the use of the [`yield`](https://docs.python.org/3/reference/expressions.html#yieldexpr ) keyword. \n",
    "\n",
    "You'll have to install MRJob: \n",
    "\n",
    "```\n",
    "pip install mrjob \n",
    "```\n",
    "\n",
    "Here we count how many words, characters, and lines are in a text. Note that his will not execute in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to run the above code from the console. The code is available in the file [word_frequency.py](word_frequency.py). Here's the (sanitized) output if we run this: \n",
    "\n",
    "```\n",
    "$python word_frequency.py pg2701.txt\n",
    "\"words\"\t215135\n",
    "\"lines\"\t22108\n",
    "\"chars\"\t1213077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's count the number of specific words**\n",
    "\n",
    "![](mapreducewc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRCountWords(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            word = re.sub(\"[^a-zA-Z]+\", \"\", word)\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCountWords.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the output for the above code looks like: \n",
    "\n",
    "```\n",
    "\"seuss\"\t2\n",
    "\"seven\"\t2\n",
    "\"sew\"\t2\n",
    "\"sews\"\t9\n",
    "\"shake\"\t2\n",
    "\"shame\"\t4\n",
    "\"she\"\t7\n",
    "\"sheep\"\t2\n",
    "\"shine\"\t1\n",
    "\"ship\"\t4\n",
    "\"shocking\"\t1\n",
    "\"shoe\"\t2\n",
    "\"shoes\"\t2\n",
    "\"shook\"\t4\n",
    "\"should\"\t15\n",
    "\"shove\"\t1\n",
    "\"show\"\t4\n",
    "\"shut\"\t2\n",
    "\"sick\"\t2\n",
    "\"side\"\t2\n",
    "\"silly\"\t1\n",
    "\"simply\"\t2\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Ngram viewer](https://books.google.com/ngrams) is an example where map reduce word freuqencies becomes useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiners\n",
    "\n",
    "This is fine, but we leave a lot of work for the shuffle and sort step. We can do better, by doing a \"local reduce\", which is known as combiners: \n",
    "\n",
    "\n",
    "![](combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the combiner has to have the same method signature as the reducer. Sometimes they can be the same, as in this example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRCountWords(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            word = re.sub(\"[^a-zA-Z]+\", \"\", word)\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    # new: the combiner\n",
    "    def combiner(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "            \n",
    "    def reducer(self, word, occurences):\n",
    "        yield word, sum(occurences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCountWords.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "How would you use MapReduce to find anagrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark can be thought of as  MapReduce 2.0\n",
    "\n",
    "- In memory as opposed to disk\n",
    "- Data can be cached in memory or disk for future use\n",
    "- 100x faster than Hadoop MapReduce in memory or 10x faster on disk\n",
    "- resilient distributed dataset (RDD)\n",
    "- Python, Java, and Scala interfaces\n",
    "- [apache-spark](http://spark.apache.org/) can be used in python through [findspark](https://github.com/minrk/findspark)\n",
    "- Easier than Hadoop while being functional, runs a general DAG\n",
    "\n",
    "For more, see Lecture 15 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k) \n",
    "and accompanying [notebook](https://github.com/cs109/2015/blob/master/Lectures/15b-Spark.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
